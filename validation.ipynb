{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import albumentations as albu\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import CloudDataset\n",
    "import segmentation_models_pytorch as smp\n",
    "from catalyst.dl.runner import SupervisedRunner\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import rle_decode, get_training_augmentation, get_preprocessing, get_validation_augmentation\n",
    "from catalyst.dl.callbacks import DiceCallback, EarlyStoppingCallback, InferCallback, CheckpointCallback\n",
    "from albumentations import pytorch as AT\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from  torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import argparse\n",
    "import torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = 'resnet50'\n",
    "encoder_weight = 'imagenet'\n",
    "path = '../input/understanding_cloud_organization'\n",
    "logdir = './logs/segmentation'\n",
    "train = pd.read_csv(f'{path}/train.csv')\n",
    "sub = pd.read_csv(f'{path}/sample_submission.csv')\n",
    "net = \"Unet\"\n",
    "class_num = 4\n",
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### should go to logging\n",
    "n_train = len(os.listdir(f'{path}/train_images'))\n",
    "n_test = len(os.listdir(f'{path}/test_images'))\n",
    "train['Image_Label'].apply(lambda x: x.split('_')[1]).value_counts()\n",
    "train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[1]).value_counts()\n",
    "train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(\n",
    "    lambda x: x.split('_')[0]).value_counts().value_counts()\n",
    "\n",
    "train['label'] = train['Image_Label'].apply(lambda x: x.split('_')[1])\n",
    "train['im_id'] = train['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "sub['label'] = sub['Image_Label'].apply(lambda x: x.split('_')[1])\n",
    "sub['im_id'] = sub['Image_Label'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "id_mask_count = train.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(\n",
    "    lambda x: x.split('_')[0]).value_counts(). \\\n",
    "    reset_index().rename(columns={'index': 'img_id', 'Image_Label': 'count'})\n",
    "train_ids, valid_ids = train_test_split(id_mask_count['img_id'].values, random_state=42,\n",
    "                                        stratify=id_mask_count['count'], test_size=0.1)\n",
    "test_ids = sub['Image_Label'].apply(lambda x: x.split('_')[0]).drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### model parameter\n",
    "ACTIVATION = torch.nn.Sigmoid\n",
    "model = getattr(smp, net)(\n",
    "    encoder_name=encoder,\n",
    "    encoder_weights=encoder_weight,\n",
    "    classes=class_num,\n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder, encoder_weight)\n",
    "model.cuda()\n",
    "# torchsummary.summary(model, (3,320, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-88a201f97f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../checkpoint/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/18\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m valid_dataset = CloudDataset(df=train, datatype='valid', img_ids=valid_ids,\n\u001b[1;32m      5\u001b[0m                              \u001b[0mtransforms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_validation_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"loading model\")\n",
    "model.load_state_dict(torch.load(\"../checkpoint/\"+net+\"/18\"))\n",
    "num_workers = 8\n",
    "valid_dataset = CloudDataset(df=train, datatype='valid', img_ids=valid_ids,\n",
    "                             transforms=get_validation_augmentation(),\n",
    "                             preprocessing=get_preprocessing(preprocessing_fn), path=path)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "loaders = {\n",
    "    \"valid\": valid_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-723c378e8036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validating\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"validating\")\n",
    "validation_loss = []\n",
    "for img, label in tqdm(loaders[\"valid\"]):\n",
    "    img = img.cuda()\n",
    "    label = label.cuda()\n",
    "    logit = model.predict(img)\n",
    "    loss = criterion(logit, label)\n",
    "    validation_loss.append(loss.item())\n",
    "\n",
    "print(\"Validation loss: \", np.mean(validation_loss))\n",
    "writer.add_scalar(\"validation_loss\", np.mean(validation_loss), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
